{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toyGPT - a character level GPT model trained on all of Shakespeare's works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of characters in text:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of characters in text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now construct our vocabulary from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique characters in text:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"length of unique characters in text: \", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the encoding and decoding dictionaries and functions that will be used to encode and decode the text. When using language models, our input text needs to be converted into tokens that can be used as input to the model. In our case we are using a very basic encoding in which each character is encoded as our vocabulary set is not too large. In larger vocabularies, a more complex encoding is required which uses subword units. This level of encoding is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {c:i for i,c in enumerate(chars)}\n",
    "itoc = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "encode = lambda str: [ctoi[char] for char in str]\n",
    "decode = lambda lst: ''.join([itoc[i] for i in lst])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can divide our dataset into training and validation sets. We will use 90% of the data for training and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(data)*0.9)\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed our data into the model we don't feed it all the data at once. That would be too computationally expensive. Instead we feed it in chunks. The size of these chunks will be called block size. Now, the important thing to note here is that when we take a chunk of the data, this chunk translates to multiple training examples. This is possible because we can look at a chunk in smaller parts and each of these parts is a training example. For example if we get a chunk that looks like this:\n",
    "\n",
    "```\n",
    "Hello World!\n",
    "```\n",
    "\n",
    "It can be split into the following training examples:\n",
    "\n",
    "```\n",
    "H -> e\n",
    "He -> l\n",
    "Hel -> l\n",
    "Hell -> o\n",
    "Hello ->\n",
    "Hello -> W\n",
    "Hello W -> o\n",
    "Hello Wo -> r\n",
    "Hello Wor -> l\n",
    "Hello Worl -> d\n",
    "Hello World -> !\n",
    "```\n",
    "\n",
    "In our case each character acts as a training example or an addition to it. So if we have a chunk of size 10, we will have 10 training examples. This is why we need to divide our dataset into chunks of size block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is: F([18]) the target is: i(47)\n",
      "When the input is: Fi([18, 47]) the target is: r(56)\n",
      "When the input is: Fir([18, 47, 56]) the target is: s(57)\n",
      "When the input is: Firs([18, 47, 56, 57]) the target is: t(58)\n",
      "When the input is: First([18, 47, 56, 57, 58]) the target is:  (1)\n",
      "When the input is: First ([18, 47, 56, 57, 58, 1]) the target is: C(15)\n",
      "When the input is: First C([18, 47, 56, 57, 58, 1, 15]) the target is: i(47)\n",
      "When the input is: First Ci([18, 47, 56, 57, 58, 1, 15, 47]) the target is: t(58)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"When the input is: {decode(context.tolist())}({context.tolist()}) the target is: {decode([target.item()])}({target})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our chunks, we want to pass in batches of these chunks to the model. This is where batch_size comes in. We will take batch_size chunks and pass them to the model. This means that we will have batch_size * block_size training examples per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is: s([57]) the target is:  (1)\n",
      "When the input is: s ([57, 1]) the target is: h(46)\n",
      "When the input is: s h([57, 1, 46]) the target is: i(47)\n",
      "When the input is: s hi([57, 1, 46, 47]) the target is: s(57)\n",
      "When the input is: s his([57, 1, 46, 47, 57]) the target is:  (1)\n",
      "When the input is: s his ([57, 1, 46, 47, 57, 1]) the target is: l(50)\n",
      "When the input is: s his l([57, 1, 46, 47, 57, 1, 50]) the target is: o(53)\n",
      "When the input is: s his lo([57, 1, 46, 47, 57, 1, 50, 53]) the target is: v(60)\n",
      "When the input is:  ([1]) the target is: t(58)\n",
      "When the input is:  t([1, 58]) the target is: h(46)\n",
      "When the input is:  th([1, 58, 46]) the target is: e(43)\n",
      "When the input is:  the([1, 58, 46, 43]) the target is: r(56)\n",
      "When the input is:  ther([1, 58, 46, 43, 56]) the target is: e(43)\n",
      "When the input is:  there([1, 58, 46, 43, 56, 43]) the target is:  (1)\n",
      "When the input is:  there ([1, 58, 46, 43, 56, 43, 1]) the target is: c(41)\n",
      "When the input is:  there c([1, 58, 46, 43, 56, 43, 1, 41]) the target is: a(39)\n",
      "When the input is: E([17]) the target is: N(26)\n",
      "When the input is: EN([17, 26]) the target is: C(15)\n",
      "When the input is: ENC([17, 26, 15]) the target is: E(17)\n",
      "When the input is: ENCE([17, 26, 15, 17]) the target is: :(10)\n",
      "When the input is: ENCE:([17, 26, 15, 17, 10]) the target is: \n",
      "(0)\n",
      "When the input is: ENCE:\n",
      "([17, 26, 15, 17, 10, 0]) the target is: T(32)\n",
      "When the input is: ENCE:\n",
      "T([17, 26, 15, 17, 10, 0, 32]) the target is: o(53)\n",
      "When the input is: ENCE:\n",
      "To([17, 26, 15, 17, 10, 0, 32, 53]) the target is:  (1)\n",
      "When the input is: s([57]) the target is: t(58)\n",
      "When the input is: st([57, 58]) the target is: ,(6)\n",
      "When the input is: st,([57, 58, 6]) the target is:  (1)\n",
      "When the input is: st, ([57, 58, 6, 1]) the target is: w(61)\n",
      "When the input is: st, w([57, 58, 6, 1, 61]) the target is: i(47)\n",
      "When the input is: st, wi([57, 58, 6, 1, 61, 47]) the target is: t(58)\n",
      "When the input is: st, wit([57, 58, 6, 1, 61, 47, 58]) the target is: h(46)\n",
      "When the input is: st, with([57, 58, 6, 1, 61, 47, 58, 46]) the target is: \n",
      "(0)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    dataset = train_data if split == 'train' else val_data\n",
    "    start_idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[idx:idx+block_size] for idx in start_idx])\n",
    "    y = torch.stack([dataset[idx+1:idx+block_size+1] for idx in start_idx])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When the input is: {decode(context.tolist())}({context.tolist()}) the target is: {decode([target.item()])}({target})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by making the most basic language model (Bigram model) and see how it performs on our dataset. We will have a token embedding table of size vocab_size by vocab_size that will encode each character. The forward pass will use the token embedding table to encode the input and calculate the loss using the cross entropy loss function. \n",
    "\n",
    "We can also use the model to generate text. We will start by giving it a prompt and then we will let it generate the rest of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "loss: 4.886534690856934\n",
      "\n",
      "o$,q&IWqW&xtCjaB?ij&bYRGkF?b; f ,CbwhtERCIfuWr,DzJERjhLlVaF&EjffPHDFcNoGIG'&$qXisWTkJPw\n",
      " ,b Xgx?D3sj\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, generate_tokens):\n",
    "        for i in range(generate_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], 1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(f\"loss: {loss.item()}\") # ideally should be -ln(1/65) = 4.174\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that without any training the model generates text that looks nothing like what we would expect. So let's train it and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5233194828033447\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "for _ in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUDUThe chas.\n",
      "F lisen tabr:\n",
      "LI mus nk,\n",
      "A: al l ayo cenghe's therinvar,\n",
      "TEsen ithawaneit at islinerainy atsomo clour pad d wikn h,\n",
      "HYy my Tholes:\n",
      "it GBy ke m vilou xthazinderand llo chee lond Cld this lisesule wars, tirofof wnofan\n",
      "Rou cthe p.\n",
      "\n",
      "By hat celis ire m, aksthethe aur withAR wotoot.\n",
      "Toy:me, of Ithed; bo r:\n",
      "DWAy celowinoourne,\n",
      "WIDYoukesu t I:f fowhilong bert irw:\n",
      "I m;\n",
      "ADWhit hor hy t I nd, billexve, war t, s\n",
      "When re llyong thm ithinde!\n",
      "Whem mire ow\n",
      "MIAPet mad, trd br hay\n",
      "ANG w t we illlaisthe:\n",
      "CESk ewhaiowaue e;I'OND:\n",
      "t m; br\n",
      "Fergho br rosoulin rfe.\n",
      "\n",
      "Lnoof by, bald woande: ay,\n",
      "LABRKitirit t, ken,\n",
      "Whisppal\n",
      "\n",
      "And, r ar st\n",
      "Blalist su s the,\n",
      "\n",
      "AUC; har anorg mellban, ll w ny hand.\n",
      "\n",
      "A s\n",
      "I thitherol at ceres, sticco:\n",
      "TI st ngere, t t!\n",
      "IZABurnosoreivet at cay iendet ch ds frthanan g ilr INGrorsto itopllver hequleat anehmoqus t cthabyowoveal Bushanean orusun,\n",
      "CO:\n",
      "!\n",
      "\n",
      "SWAUESow Sore t'SThomasth cor:\n",
      "FO:\n",
      "GOxt\n",
      "Wherieatrerpethalfll t. fit RL:\n",
      "I inondvedat ir'd icere.\n",
      "Ben olan, te ENToullo ford \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 1000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10,000 steps of optmization we see how much better the model has gotten and the text it generates is much more of what we would expect from Shakespeare. However, due to the simplicity of the model, it still has a lot of trouble with generating words and sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical trick for self-attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to move beyond using just the previous character to inform the next character. A simple improvement would be to use the average of all previous embeddings in our context. One way to perform this is to calculate the average using a for loop as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0894, -0.4926])\n",
      "tensor([-0.0894, -0.4926])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn((B,T,C))\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow[b, t] = torch.mean(x[b, :t+1], 0)\n",
    "\n",
    "print((x[0,0]+x[0,1])/2)\n",
    "print(xbow[0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see how the for loop can be used to calculate the average until time t. However, this is not very efficient and we can use a mathematical trick to improve the efficiency. We will be using matrix multiplication which will vectorize the operation and make it much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones((T,T)))\n",
    "weights /= weights.sum(1, keepdim=True)\n",
    "xbow2 = weights @ x\n",
    "print(xbow[0])\n",
    "print(xbow2[0])\n",
    "print(torch.allclose(xbow, xbow2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tril method gives us a lower triangular matrix from the input matrix. And when we divide each row in it by the sum of the row, we get weights that will sum to 1 and can therefore be use to calculate the weighted average of the embeddings. By getting the lower triangular matrix we are making sure that we are only using the embeddings until time t and not the ones after it.\n",
    "\n",
    "Now we can also obtain the weights matrix another way that may seem more intuitive. Let's implement that and talk about the intuition behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T)))\n",
    "weights = torch.zeros((T,T))\n",
    "weights = torch.masked_fill(weights, tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, 1)\n",
    "xbow3 = weights @ x\n",
    "print(xbow[0])\n",
    "print(xbow3[0])\n",
    "print(torch.allclose(xbow, xbow3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this approach makes more intuitive sense than the previous implementation is because we can view the initialization of the weights as being the connections between the current embedding and the previous embeddings. So initially we set the weights to all previous embeddings to be 0 and to all future embeddings to be -inf. Then as the model learns, it can adjust the weights to be more appropriate. This is why we use the softmax function to get the weights as it will make sure that the weights sum to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
